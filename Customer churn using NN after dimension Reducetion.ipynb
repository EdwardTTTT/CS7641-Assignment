{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf6dba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import copy\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.compose import ColumnTransformer\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "831f69bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1 = pd.read_csv('churn.csv')\n",
    "# Preprocess the data\n",
    "data_prep = df1.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\n",
    "categorical_features = ['Geography', 'Gender']\n",
    "numerical_features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "X = data_prep.drop('Exited', axis=1)\n",
    "y = data_prep['Exited']\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components= 2).fit(X_preprocessed)\n",
    "X_pca = pca.transform(X_preprocessed)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ef49a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the neural network architecture\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d06b1675",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training and evaluation function\n",
    "def train_and_evaluate_model(params, X_train, y_train, X_test, y_test, device):\n",
    "    hidden_size, learning_rate, batch_size = params\n",
    "    model = NeuralNetwork(X_train.shape[1], hidden_size).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.to_numpy()).to(device)\n",
    "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "    \n",
    "    dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, targets in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor).squeeze()\n",
    "        y_pred_proba = outputs.cpu().numpy()\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    return fpr, tpr, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a22a242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e76ff195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AUC: 0.6992680078018688\n",
      "Best hyperparameters: {'hidden_size': 100, 'learning_rate': 0.01, 'batch_size': 128, 'num_epochs': 5}\n"
     ]
    }
   ],
   "source": [
    "#use GS to find best hyper parameter comb\n",
    "def evaluate_model_auc(params, X_train, y_train, X_test, y_test, device):\n",
    "    model = NeuralNetwork(X_train.shape[1], params['hidden_size']).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "    \n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.to_numpy()).to(device)\n",
    "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "    \n",
    "    dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "    \n",
    "    num_epochs = params['num_epochs']\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, targets in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor).squeeze()\n",
    "        y_pred_proba = outputs.cpu().numpy()\n",
    "    \n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    return auc_score\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'hidden_size': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'batch_size': [128, 256],\n",
    "    'num_epochs': [5]  \n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_combinations = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "best_score = 0\n",
    "best_params = None\n",
    "for params in param_combinations:\n",
    "    score = evaluate_model_auc(params, X_train, y_train, X_test, y_test, device)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best AUC: {best_score}\")\n",
    "print(f\"Best hyperparameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de9bcfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC with Best Hyperparameters: 0.7032653315794248\n"
     ]
    }
   ],
   "source": [
    "best_params = {\n",
    "    'hidden_size': 100, \n",
    "    'learning_rate': 0.01,\n",
    "    'batch_size': 128,\n",
    "    'num_epochs': 5 \n",
    "}\n",
    "\n",
    "# Train the model with the best hyperparameters and evaluate on the test set\n",
    "test_auc = evaluate_model_auc(best_params, X_train, y_train, X_test, y_test, device)\n",
    "\n",
    "print(f\"Test AUC with Best Hyperparameters: {test_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a665a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c47b23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (8000, 13) Positive examples:  1644\n",
      "Test:  (2000, 13) Positive examples:  393\n",
      "Index(['RowNumber', 'CustomerId', 'Surname', 'CreditScore', 'Geography',\n",
      "       'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n",
      "       'IsActiveMember', 'EstimatedSalary', 'Exited'],\n",
      "      dtype='object')\n",
      "Feature: 0, Score: 0.01605\n",
      "Feature: 1, Score: 0.40014\n",
      "Feature: 2, Score: 0.00076\n",
      "Feature: 3, Score: 0.04079\n",
      "Feature: 4, Score: 0.29500\n",
      "Feature: 5, Score: 0.00027\n",
      "Feature: 6, Score: 0.10943\n",
      "Feature: 7, Score: 0.00524\n",
      "Feature: 8, Score: 0.02211\n",
      "Feature: 9, Score: 0.06027\n",
      "Feature: 10, Score: 0.00859\n",
      "Feature: 11, Score: 0.01756\n",
      "Feature: 12, Score: 0.02380\n",
      "[0.0002681635905900325, 0.0007614354612305828, 0.005236656890911803, 0.00859231085285973, 0.016045863815106132, 0.017557621787925096, 0.02210892876117666, 0.023798909244274503, 0.04079388593147097, 0.06026951548306544, 0.10943467599359048, 0.29499533970203184, 0.40013669248576683]\n",
      "[False False False False False False False False False  True  True  True\n",
      "  True]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Random forest \n",
    "df1 = pd.read_csv('churn.csv')\n",
    "# Preprocess the data\n",
    "data_prep = df1.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\n",
    "categorical_features = ['Geography', 'Gender']\n",
    "numerical_features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "X = data_prep.drop('Exited', axis=1)\n",
    "y = data_prep['Exited']\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "\n",
    "X_train1_b, X_test1_b, y_train1_b, y_test1_b = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n",
    "X_train1, X_test1, y_train1, y_test1 = X_train1_b, X_test1_b, y_train1_b, y_test1_b\n",
    "y_train1_b.sum(), y_test1_b.sum()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train1_b)\n",
    "scaler.mean_\n",
    "X_train1 = scaler.transform(X_train1_b)\n",
    "print(\"Train: \", X_train1.shape, \"Positive examples: \", y_train1.sum())\n",
    "\n",
    "X_test1 = scaler.transform(X_test1_b)\n",
    "print(\"Test: \", X_test1.shape, \"Positive examples: \", y_test1.sum())\n",
    "\n",
    "df2 = df1.copy()\n",
    "print(df1.columns)\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=3)\n",
    "model.fit(X_train1, y_train1)\n",
    "importance = model.feature_importances_\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "importance = sorted(importance)\n",
    "print(importance)\n",
    "# Feature  (CustomerId,Geography,Gender,HasCrCard)\n",
    "mask_d1 = np.array(importance)>0.05\n",
    "print(mask_d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb40d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=pd.read_csv(\"churn.csv\")\n",
    "DF_RF = df3[['CustomerId', 'Geography', 'Gender', 'HasCrCard','Exited']]\n",
    "# Replace categorical variables with numeric values\n",
    "# Preprocess the data\n",
    "data_prep = DF_RF.drop([ 'CustomerId'], axis=1)\n",
    "categorical_features = ['Geography', 'Gender']\n",
    "numerical_features = ['HasCrCard']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "X = data_prep.drop('Exited', axis=1)\n",
    "y = data_prep['Exited']\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2985256b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AUC: 0.6366337005579243\n",
      "Best hyperparameters: {'hidden_size': 50, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 5}\n"
     ]
    }
   ],
   "source": [
    "# Define the grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'hidden_size': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'batch_size': [128, 256],\n",
    "    'num_epochs': [5]  \n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_combinations = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "best_score = 0\n",
    "best_params = None\n",
    "for params in param_combinations:\n",
    "    score = evaluate_model_auc(params, X_train, y_train, X_test, y_test, device)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best AUC: {best_score}\")\n",
    "print(f\"Best hyperparameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70e46a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC with Best Hyperparameters: 0.6319736998775288\n"
     ]
    }
   ],
   "source": [
    "best_params = {\n",
    "    'hidden_size': 50, \n",
    "    'learning_rate': 0.1,\n",
    "    'batch_size': 128,\n",
    "    'num_epochs': 5 \n",
    "}\n",
    "\n",
    "# Train the model with the best hyperparameters and evaluate on the test set\n",
    "test_auc = evaluate_model_auc(best_params, X_train, y_train, X_test, y_test, device)\n",
    "\n",
    "print(f\"Test AUC with Best Hyperparameters: {test_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "966639e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from scipy.stats import pointbiserialr, chi2_contingency\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "import itertools\n",
    "import matplotlib.backends.backend_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0c93bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Balance', 'CustomerId', 'Age', 'Tenure', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'CreditScore', 'RowNumber']\n"
     ]
    }
   ],
   "source": [
    "df3=pd.read_csv(\"churn.csv\")\n",
    "# Exclude the target variable from the clustering\n",
    "df_cluster = df3.drop(columns=['Exited','Geography', 'Gender','Surname'])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = df_cluster.corr().abs()\n",
    "corr_matrix = corr_matrix.clip(lower=-1, upper=1)\n",
    "\n",
    "# Turn the correlation matrix into a distance matrix\n",
    "dist_matrix = 1 - corr_matrix\n",
    "\n",
    "# Perform hierarchical/agglomerative clustering\n",
    "clusters = linkage(squareform(dist_matrix), method=\"average\")\n",
    "\n",
    "# Form flat clusters from the hierarchical clustering defined by the linkage matrix\n",
    "num_clusters = df_cluster.shape[1] // 3\n",
    "cluster_labels = fcluster(clusters, num_clusters, criterion=\"maxclust\")\n",
    "\n",
    "# Select the most representative variable from each cluster\n",
    "selected_features = []\n",
    "for i in range(1, num_clusters + 1):\n",
    "    cluster_vars = [\n",
    "        var\n",
    "        for var, cluster in zip(df_cluster.columns, cluster_labels) \n",
    "        if cluster == i\n",
    "    ]\n",
    "\n",
    "    # Select the variable with the highest sum of correlations with other variables in the cluster\n",
    "    var_correlations = corr_matrix.loc[cluster_vars, cluster_vars].sum()\n",
    "    most_representative = var_correlations.idxmax()\n",
    "    selected_features.append(most_representative)\n",
    "    # Add other variables in the cluster that have a correlation less than 50% with the most representative variable \n",
    "    for var in cluster_vars:\n",
    "         if (\n",
    "            var != most_representative\n",
    "            and corr_matrix.loc[most_representative, var] < 0.5 \n",
    "        ):\n",
    "            selected_features.append(var)\n",
    "\n",
    "# Update the Dataframe to include only the selected features, along with the target variable\n",
    "    #df3 = df3[selected_features + 'Exited']\n",
    "print(selected_features)\n",
    "\n",
    "df_clustered=df3[['Geography', 'Gender','Balance', 'CustomerId', 'Age', 'Tenure', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'CreditScore', 'RowNumber','Exited']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dbb31f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace categorical variables with numeric values\n",
    "# Preprocess the data\n",
    "data_prep = df_clustered.drop([ 'CustomerId', 'RowNumber'], axis=1)\n",
    "categorical_features = ['Geography', 'Gender']\n",
    "numerical_features = ['Balance', 'Age', 'Tenure', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'CreditScore']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "X = data_prep.drop('Exited', axis=1)\n",
    "y = data_prep['Exited']\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f33eb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_auc(params, X_train, y_train, X_test, y_test, device):\n",
    "    model = NeuralNetwork(X_train.shape[1], params['hidden_size']).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "    \n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.to_numpy()).to(device)\n",
    "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "    \n",
    "    dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "    \n",
    "    num_epochs = params['num_epochs']\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, targets in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor).squeeze()\n",
    "        y_pred_proba = outputs.cpu().numpy()\n",
    "    \n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    return auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "467d49b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AUC: 0.859242464619432\n",
      "Best hyperparameters: {'hidden_size': 100, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 5}\n"
     ]
    }
   ],
   "source": [
    "# Define the grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'hidden_size': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'batch_size': [128, 256],\n",
    "    'num_epochs': [5]  \n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_combinations = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "best_score = 0\n",
    "best_params = None\n",
    "for params in param_combinations:\n",
    "    score = evaluate_model_auc(params, X_train, y_train, X_test, y_test, device)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best AUC: {best_score}\")\n",
    "print(f\"Best hyperparameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ddfe426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC with Best Hyperparameters: 0.8576095153315795\n"
     ]
    }
   ],
   "source": [
    "best_params = {\n",
    "    'hidden_size': 100, \n",
    "    'learning_rate': 0.1,\n",
    "    'batch_size': 128,\n",
    "    'num_epochs': 5 \n",
    "}\n",
    "\n",
    "# Train the model with the best hyperparameters and evaluate on the test set\n",
    "test_auc = evaluate_model_auc(best_params, X_train, y_train, X_test, y_test, device)\n",
    "\n",
    "print(f\"Test AUC with Best Hyperparameters: {test_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de424148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
